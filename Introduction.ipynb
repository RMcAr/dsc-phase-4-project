{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Classification using Tweets on Apple and Google Products\n",
    "\n",
    "Ryan McArthur, Flatiron School DS 081720\n",
    "Module 4 Final Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this investigation, we are tasked with building a model that is able to effectively sort tweets based upon the sentiment of the text. We are provided with a dataset containing content, product in mention, and emotion contained within the tweet. Our target for this investigation will the emotion contained within the text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fsds v0.2.27 loaded.  Read the docs: https://fs-ds.readthedocs.io/en/latest/ \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style  type=\"text/css\" >\n",
       "</style><table id=\"T_d3ed4b46_408d_11eb_88de_144f8ac5640c\" ><caption>Loaded Packages and Handles</caption><thead>    <tr>        <th class=\"col_heading level0 col0\" >Handle</th>        <th class=\"col_heading level0 col1\" >Package</th>        <th class=\"col_heading level0 col2\" >Description</th>    </tr></thead><tbody>\n",
       "                <tr>\n",
       "                                <td id=\"T_d3ed4b46_408d_11eb_88de_144f8ac5640crow0_col0\" class=\"data row0 col0\" >dp</td>\n",
       "                        <td id=\"T_d3ed4b46_408d_11eb_88de_144f8ac5640crow0_col1\" class=\"data row0 col1\" >IPython.display</td>\n",
       "                        <td id=\"T_d3ed4b46_408d_11eb_88de_144f8ac5640crow0_col2\" class=\"data row0 col2\" >Display modules with helpful display and clearing commands.</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                                <td id=\"T_d3ed4b46_408d_11eb_88de_144f8ac5640crow1_col0\" class=\"data row1 col0\" >fs</td>\n",
       "                        <td id=\"T_d3ed4b46_408d_11eb_88de_144f8ac5640crow1_col1\" class=\"data row1 col1\" >fsds</td>\n",
       "                        <td id=\"T_d3ed4b46_408d_11eb_88de_144f8ac5640crow1_col2\" class=\"data row1 col2\" >Custom data science bootcamp student package</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                                <td id=\"T_d3ed4b46_408d_11eb_88de_144f8ac5640crow2_col0\" class=\"data row2 col0\" >mpl</td>\n",
       "                        <td id=\"T_d3ed4b46_408d_11eb_88de_144f8ac5640crow2_col1\" class=\"data row2 col1\" >matplotlib</td>\n",
       "                        <td id=\"T_d3ed4b46_408d_11eb_88de_144f8ac5640crow2_col2\" class=\"data row2 col2\" >Matplotlib's base OOP module with formatting artists</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                                <td id=\"T_d3ed4b46_408d_11eb_88de_144f8ac5640crow3_col0\" class=\"data row3 col0\" >plt</td>\n",
       "                        <td id=\"T_d3ed4b46_408d_11eb_88de_144f8ac5640crow3_col1\" class=\"data row3 col1\" >matplotlib.pyplot</td>\n",
       "                        <td id=\"T_d3ed4b46_408d_11eb_88de_144f8ac5640crow3_col2\" class=\"data row3 col2\" >Matplotlib's matlab-like plotting module</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                                <td id=\"T_d3ed4b46_408d_11eb_88de_144f8ac5640crow4_col0\" class=\"data row4 col0\" >np</td>\n",
       "                        <td id=\"T_d3ed4b46_408d_11eb_88de_144f8ac5640crow4_col1\" class=\"data row4 col1\" >numpy</td>\n",
       "                        <td id=\"T_d3ed4b46_408d_11eb_88de_144f8ac5640crow4_col2\" class=\"data row4 col2\" >scientific computing with Python</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                                <td id=\"T_d3ed4b46_408d_11eb_88de_144f8ac5640crow5_col0\" class=\"data row5 col0\" >pd</td>\n",
       "                        <td id=\"T_d3ed4b46_408d_11eb_88de_144f8ac5640crow5_col1\" class=\"data row5 col1\" >pandas</td>\n",
       "                        <td id=\"T_d3ed4b46_408d_11eb_88de_144f8ac5640crow5_col2\" class=\"data row5 col2\" >High performance data structures and tools</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                                <td id=\"T_d3ed4b46_408d_11eb_88de_144f8ac5640crow6_col0\" class=\"data row6 col0\" >sns</td>\n",
       "                        <td id=\"T_d3ed4b46_408d_11eb_88de_144f8ac5640crow6_col1\" class=\"data row6 col1\" >seaborn</td>\n",
       "                        <td id=\"T_d3ed4b46_408d_11eb_88de_144f8ac5640crow6_col2\" class=\"data row6 col2\" >High-level data visualization library based on matplotlib</td>\n",
       "            </tr>\n",
       "    </tbody></table>"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x163a92871d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[i] Pandas .iplot() method activated.\n"
     ]
    }
   ],
   "source": [
    "# we import all necessary tools\n",
    "\n",
    "!pip install -U fsds\n",
    "\n",
    "from fsds.imports import * \n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize, FreqDist\n",
    "\n",
    "import string\n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data\n",
    "\n",
    "We first need to understand our data, complete some basic cleaning, and check for any issues that would prevent future modelling. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9093"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we import and inspect our dataframe\n",
    "\n",
    "df = pd.read_csv(\"data/NLP_dataset.csv\", encoding = 'unicode_escape')\n",
    "\n",
    "\n",
    "\n",
    "#df.describe()\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>product</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>.@wesley83 I have a 3G iPhone. After 3 hrs twe...</td>\n",
       "      <td>iPhone</td>\n",
       "      <td>Negative emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@jessedee Know about @fludapp ? Awesome iPad/i...</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@swonderlin Can not wait for #iPad 2 also. The...</td>\n",
       "      <td>iPad</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@sxsw I hope this year's festival isn't as cra...</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>Negative emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@sxtxstate great stuff on Fri #SXSW: Marissa M...</td>\n",
       "      <td>Google</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text             product  \\\n",
       "0  .@wesley83 I have a 3G iPhone. After 3 hrs twe...              iPhone   \n",
       "1  @jessedee Know about @fludapp ? Awesome iPad/i...  iPad or iPhone App   \n",
       "2  @swonderlin Can not wait for #iPad 2 also. The...                iPad   \n",
       "3  @sxsw I hope this year's festival isn't as cra...  iPad or iPhone App   \n",
       "4  @sxtxstate great stuff on Fri #SXSW: Marissa M...              Google   \n",
       "\n",
       "          sentiment  \n",
       "0  Negative emotion  \n",
       "1  Positive emotion  \n",
       "2  Positive emotion  \n",
       "3  Negative emotion  \n",
       "4  Positive emotion  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we rename columns for ease of use\n",
    "\n",
    "rename_dict = {\"tweet_text\" : 'text',\n",
    "              'emotion_in_tweet_is_directed_at' : 'product',\n",
    "              'is_there_an_emotion_directed_at_a_brand_or_product' : 'sentiment'}\n",
    "\n",
    "df = df.rename(columns = rename_dict)\n",
    "\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NaN                                5802\n",
       "iPad                                946\n",
       "Apple                               661\n",
       "iPad or iPhone App                  470\n",
       "Google                              430\n",
       "iPhone                              297\n",
       "Other Google product or service     293\n",
       "Android App                          81\n",
       "Android                              78\n",
       "Other Apple product or service       35\n",
       "Name: product, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we check for missing values in our columns\n",
    "\n",
    "df['product'].value_counts(dropna = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have missing values in our product column. Of 9093 texts, 5802 did not have a product in mention, according to the data provided to us. To see if we should complete any furuther work on this, let's check how it relates to our sentiment column. If there is a sufficient mix of classes in the non-product tweets, we'll try to classify further. If there is strong class imbalance, trying to attribute products to more tweets would not inform our model in the long run. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "No emotion toward brand or product    5298\n",
       "Positive emotion                       306\n",
       "I can't tell                           147\n",
       "Negative emotion                        51\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we investigate the sentiment of tweets with no product in mention\n",
    "\n",
    "no_product_df = df.where(df['product'].isna() == True)\n",
    "\n",
    "no_product_df['sentiment'].value_counts()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of the 5802 texts that had no product in mention, 5445 were neutral, or 94% of entries. Attempting to process our data further to fill in some missing values in our product column would not serve our model best, as these entries are overwhelmingly neutral. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "No emotion toward brand or product    5389\n",
       "Positive emotion                      2978\n",
       "Negative emotion                       570\n",
       "I can't tell                           156\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we return to checking and cleaning sentiment column\n",
    "\n",
    "df['sentiment'].value_counts(dropna = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "neutral     5545\n",
       "positive    2978\n",
       "negative     570\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we clean 'sentiment' column\n",
    "\n",
    "def sentiment_cleaner(df, column):\n",
    "    sentiment_list = []\n",
    "    for i in df[column]:\n",
    "        if i == \"No emotion toward brand or product\":\n",
    "            sentiment_list.append('neutral')\n",
    "        elif i == \"I can't tell\":\n",
    "            sentiment_list.append('neutral')\n",
    "        elif i == \"Positive emotion\":\n",
    "            sentiment_list.append('positive')\n",
    "        elif i == \"Negative emotion\":\n",
    "            sentiment_list.append('negative') \n",
    "    df['sentiment'] = sentiment_list\n",
    "    return df\n",
    "\n",
    "df = sentiment_cleaner(df, 'sentiment')\n",
    "df['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>product</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>.@wesley83 I have a 3G iPhone. After 3 hrs twe...</td>\n",
       "      <td>iPhone</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@jessedee Know about @fludapp ? Awesome iPad/i...</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@swonderlin Can not wait for #iPad 2 also. The...</td>\n",
       "      <td>iPad</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@sxsw I hope this year's festival isn't as cra...</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@sxtxstate great stuff on Fri #SXSW: Marissa M...</td>\n",
       "      <td>Google</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text             product  \\\n",
       "0  .@wesley83 I have a 3G iPhone. After 3 hrs twe...              iPhone   \n",
       "1  @jessedee Know about @fludapp ? Awesome iPad/i...  iPad or iPhone App   \n",
       "2  @swonderlin Can not wait for #iPad 2 also. The...                iPad   \n",
       "3  @sxsw I hope this year's festival isn't as cra...  iPad or iPhone App   \n",
       "4  @sxtxstate great stuff on Fri #SXSW: Marissa M...              Google   \n",
       "\n",
       "  sentiment  \n",
       "0  negative  \n",
       "1  positive  \n",
       "2  positive  \n",
       "3  negative  \n",
       "4  positive  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we inspect our df after completing our introductory cleaning \n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Preprocessing\n",
    "\n",
    "In order to generate a well-performing model, we must process our text data to a modellable state. We will accomplish this by:\n",
    "\n",
    "- Removing stopwords\n",
    "- Stemming text data\n",
    "\n",
    "Important to note is that, for Tweets, links must be removed along with hashtags and other types of characters that are unique to this type of media. Because of the uniqueness of tweets, we have to construct some 'stopwords' on our own, with the help of regular expressions. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we incorporate a regex function to clean our 'text' column\n",
    "\n",
    "def regex_cleaner(text):\n",
    "    \n",
    "    # we remove hashtags\n",
    "    text = re.sub(r'#', '', text)\n",
    "\n",
    "    # we remove hyperlinks\n",
    "    \n",
    "    # sourced through google lookup\n",
    "    \n",
    "    alt_url_regex =  r'https?:\\/\\/(www\\.)?[-a-zA-Z0–9@:%._\\+~#=]{2,256}\\.[a-z]{2,6}\\b([-a-zA-Z0–9@:%_\\+.~#?&//=]*)'\n",
    "    url_regex = r'(?:<\\w+.*?>|[^=!:\\'\\\"/]|^)((?:https?://|www\\.)[-\\w]+(?:\\.[-\\w]+)*(?::\\d+)?(?:/(?:(?:[~\\w\\+%-]|(?:[,.;@:][^\\s$]))+)?)*(?:\\?[\\w\\+%&=.;:-]+)?(?:\\#[\\w\\-\\.]*)?)(?:\\p{P}|\\s|<|$)'\n",
    "    \n",
    "    text = re.sub(r'https?:\\/\\/(www\\.)?[-a-zA-Z0–9@:%._\\+~#=]{2,256}\\.[a-z]{2,6}\\b([-a-zA-Z0–9@:%_\\+.~#?&//=]*)', '', text, flags=re.MULTILINE)\n",
    "    text = re.sub(alt_url_regex, '', text, flags=re.MULTILINE) \n",
    "    \n",
    "    return text\n",
    "\n",
    "clean_text = []\n",
    "for i in df['text']:\n",
    "    cleaned = regex_cleaner(str(i))\n",
    "    clean_text.append(cleaned)\n",
    "    \n",
    "df['text'] = clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we generate a list of all tweets, convert to a bag of words and tokenize\n",
    "\n",
    "full_list = df['text'].to_list()\n",
    "\n",
    "raw_bag = ','.join(map(str, full_list))\n",
    "\n",
    "raw_tokens = word_tokenize(raw_bag)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(',', 12552),\n",
       " ('@', 7194),\n",
       " ('mention', 7119),\n",
       " ('.', 4891),\n",
       " ('SXSW', 4731),\n",
       " ('sxsw', 4470),\n",
       " ('link', 4311),\n",
       " ('}', 4298),\n",
       " ('{', 4296),\n",
       " ('the', 3928),\n",
       " ('to', 3519),\n",
       " ('RT', 2947),\n",
       " ('at', 2859),\n",
       " (';', 2800),\n",
       " ('&', 2707),\n",
       " ('for', 2440),\n",
       " ('!', 2398),\n",
       " ('a', 2174),\n",
       " ('Google', 2134),\n",
       " ('iPad', 2116),\n",
       " (':', 2055),\n",
       " ('Apple', 1880),\n",
       " ('in', 1830),\n",
       " ('quot', 1696),\n",
       " ('of', 1691),\n",
       " ('?', 1655),\n",
       " ('is', 1649),\n",
       " ('and', 1526),\n",
       " ('I', 1461),\n",
       " ('iPhone', 1300),\n",
       " ('on', 1271),\n",
       " (\"'s\", 1232),\n",
       " ('2', 1100),\n",
       " ('store', 1049),\n",
       " ('-', 972),\n",
       " ('you', 944),\n",
       " ('Austin', 900),\n",
       " ('an', 853),\n",
       " ('amp', 836),\n",
       " ('with', 805),\n",
       " (')', 801),\n",
       " ('up', 778),\n",
       " ('(', 770),\n",
       " ('it', 767),\n",
       " ('my', 711),\n",
       " ('app', 630),\n",
       " ('...', 591),\n",
       " ('Circles', 589),\n",
       " ('new', 566),\n",
       " ('be', 544),\n",
       " ('New', 519),\n",
       " ('from', 505),\n",
       " ('this', 496),\n",
       " ('by', 485),\n",
       " ('The', 483),\n",
       " (\"n't\", 479),\n",
       " ('out', 478),\n",
       " ('that', 468),\n",
       " ('are', 456),\n",
       " ('google', 448),\n",
       " ('Android', 443),\n",
       " ('your', 431),\n",
       " ('not', 427),\n",
       " ('Store', 423),\n",
       " ('apple', 416),\n",
       " ('have', 414),\n",
       " ('via', 397),\n",
       " ('Social', 382),\n",
       " ('about', 373),\n",
       " ('line', 354),\n",
       " ('just', 345),\n",
       " ('launch', 341),\n",
       " ('me', 328),\n",
       " ('will', 327),\n",
       " ('today', 324),\n",
       " ('pop-up', 319),\n",
       " ('Launch', 312),\n",
       " ('Network', 311),\n",
       " ('do', 311),\n",
       " ('now', 306),\n",
       " ('get', 292),\n",
       " ('Called', 280),\n",
       " ('free', 276),\n",
       " ('party', 274),\n",
       " ('one', 272),\n",
       " ('like', 269),\n",
       " ('Major', 269),\n",
       " ('or', 269),\n",
       " ('has', 268),\n",
       " (\"'re\", 265),\n",
       " ('social', 265),\n",
       " ('iPad2', 264),\n",
       " ('but', 264),\n",
       " ('It', 260),\n",
       " ('they', 258),\n",
       " ('all', 256),\n",
       " (\"'m\", 254),\n",
       " ('ipad', 251),\n",
       " ('Today', 250),\n",
       " ('time', 242)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we inspect the frequency distribution of our raw tokens\n",
    "\n",
    "frequency = FreqDist(raw_tokens)\n",
    "frequency.most_common(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we generate a list of stopwords and define a function to remove stopwords from a bag of words\n",
    "\n",
    "stops = (stopwords.words('english') +\n",
    "        list(string.punctuation) +\n",
    "        ['“','”','...',\"''\",'’','``'] + \n",
    "        [\"rt\", \"'s\", \"n't\", \"'re\", \"'m\", \"'ll\", \"--\", \"..\",'link', 'mention', 'austin'] +\n",
    "        [str(i) for i in range(0, 10)])\n",
    "\n",
    "         \n",
    "def rem_stops(bag, stopwords = stops):\n",
    "         stops_removed = [word.lower() for word in bag if word.lower() not in stopwords]\n",
    "         return stops_removed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we define a function to stem our data\n",
    "\n",
    "from nltk.stem.porter import *\n",
    "\n",
    "\n",
    "def stemmer(bag):\n",
    "    stemmer = PorterStemmer()\n",
    "    stem_bag = []\n",
    "    for word in bag:\n",
    "        stem_bag.append(stemmer.stem(word))\n",
    "    return stem_bag\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('sxsw', 9402),\n",
       " ('googl', 2597),\n",
       " ('ipad', 2520),\n",
       " ('appl', 2303),\n",
       " ('quot', 1702),\n",
       " ('iphon', 1522),\n",
       " ('store', 1511),\n",
       " ('new', 1127),\n",
       " ('app', 1034),\n",
       " ('launch', 838),\n",
       " ('amp', 836),\n",
       " ('circl', 673),\n",
       " ('social', 650),\n",
       " ('thi', 613),\n",
       " ('today', 574),\n",
       " ('android', 572),\n",
       " ('open', 533),\n",
       " ('get', 529),\n",
       " ('network', 487),\n",
       " ('ipad2', 471),\n",
       " ('line', 453),\n",
       " ('go', 423),\n",
       " ('pop-up', 422),\n",
       " ('via', 413),\n",
       " ('call', 403),\n",
       " ('parti', 392),\n",
       " ('free', 387),\n",
       " ('mobil', 341),\n",
       " ('sxswi', 339),\n",
       " ('come', 331),\n",
       " ('like', 324),\n",
       " ('use', 319),\n",
       " ('ha', 313),\n",
       " ('time', 312),\n",
       " ('one', 312),\n",
       " ('major', 306),\n",
       " ('win', 299),\n",
       " ('map', 277),\n",
       " ('temporari', 264),\n",
       " ('day', 263),\n",
       " ('check', 259),\n",
       " ('possibl', 255),\n",
       " ('see', 254),\n",
       " ('need', 248),\n",
       " ('wa', 246),\n",
       " ('look', 234),\n",
       " ('design', 233),\n",
       " ('make', 227),\n",
       " ('peopl', 226),\n",
       " ('downtown', 225),\n",
       " ('great', 222),\n",
       " ('mayer', 215),\n",
       " ('popup', 210),\n",
       " ('know', 195),\n",
       " ('set', 194),\n",
       " ('talk', 192),\n",
       " ('marissa', 189),\n",
       " ('think', 189),\n",
       " ('want', 187),\n",
       " ('got', 185),\n",
       " ('show', 184),\n",
       " ('say', 182),\n",
       " ('w/', 182),\n",
       " ('onli', 175),\n",
       " ('good', 175),\n",
       " ('love', 174),\n",
       " ('thank', 174),\n",
       " ('\\x89ûï', 169),\n",
       " ('pop', 169),\n",
       " ('product', 167),\n",
       " ('first', 166),\n",
       " ('us', 162),\n",
       " ('take', 159),\n",
       " ('search', 157),\n",
       " ('year', 156),\n",
       " ('game', 156),\n",
       " ('befor', 156),\n",
       " ('panel', 156),\n",
       " ('user', 155),\n",
       " ('guy', 152),\n",
       " ('rumor', 150),\n",
       " ('music', 148),\n",
       " ('next', 147),\n",
       " ('cool', 147),\n",
       " ('shop', 146),\n",
       " ('tweet', 144),\n",
       " ('awesom', 144),\n",
       " ('tech', 144),\n",
       " ('best', 144),\n",
       " ('download', 143),\n",
       " ('work', 140),\n",
       " ('ani', 139),\n",
       " ('give', 137),\n",
       " ('video', 135),\n",
       " ('wait', 134),\n",
       " ('would', 134),\n",
       " ('big', 132),\n",
       " ('buy', 128),\n",
       " ('around', 127),\n",
       " ('last', 125),\n",
       " ('doe', 125),\n",
       " ('dure', 124),\n",
       " ('thing', 124),\n",
       " ('updat', 123),\n",
       " ('session', 122),\n",
       " ('case', 121),\n",
       " ('right', 120),\n",
       " ('anyon', 119),\n",
       " ('hi', 119),\n",
       " ('phone', 119),\n",
       " ('gt', 117),\n",
       " ('head', 117),\n",
       " ('whi', 117),\n",
       " ('news', 116),\n",
       " (\"'ve\", 115),\n",
       " ('even', 114),\n",
       " ('locat', 113),\n",
       " ('japan', 113),\n",
       " ('includ', 112),\n",
       " ('realli', 110),\n",
       " ('still', 107),\n",
       " ('photo', 106),\n",
       " ('start', 103),\n",
       " ('find', 103),\n",
       " ('live', 103),\n",
       " ('booth', 103),\n",
       " ('futur', 102),\n",
       " ('market', 102),\n",
       " ('team', 101),\n",
       " ('twitter', 101),\n",
       " ('event', 100),\n",
       " ('hey', 100),\n",
       " ('u', 100),\n",
       " ('congress', 100),\n",
       " ('way', 100),\n",
       " ('sell', 99),\n",
       " ('develop', 99),\n",
       " ('week', 99),\n",
       " ('itun', 98),\n",
       " ('blackberri', 98),\n",
       " ('digit', 96),\n",
       " ('world', 95),\n",
       " ('heard', 95),\n",
       " ('present', 95),\n",
       " ('tri', 94),\n",
       " ('could', 94),\n",
       " ('6th', 94),\n",
       " ('bing', 94),\n",
       " ('veri', 92),\n",
       " ('save', 92),\n",
       " ('let', 91),\n",
       " ('featur', 90),\n",
       " ('technolog', 90),\n",
       " ('everyon', 89),\n",
       " ('tonight', 88),\n",
       " ('may', 87),\n",
       " ('code', 87),\n",
       " ('play', 85),\n",
       " ('interest', 85),\n",
       " ('tv', 85),\n",
       " ('nice', 84),\n",
       " ('friend', 84),\n",
       " ('back', 84),\n",
       " ('meet', 83),\n",
       " ('fun', 83),\n",
       " ('interact', 82),\n",
       " ('facebook', 82),\n",
       " ('geek', 82),\n",
       " ('connect', 81),\n",
       " ('becaus', 81),\n",
       " ('ca', 80),\n",
       " ('lot', 80),\n",
       " ('also', 79),\n",
       " ('mani', 79),\n",
       " ('hour', 78),\n",
       " ('ever', 78),\n",
       " ('web', 78),\n",
       " ('releas', 78),\n",
       " ('away', 77),\n",
       " ('temp', 76),\n",
       " ('keep', 76),\n",
       " ('hope', 75),\n",
       " ('person', 75),\n",
       " ('pleas', 75),\n",
       " ('pic', 74),\n",
       " ('share', 74),\n",
       " ('sxsw.', 73),\n",
       " ('long', 73),\n",
       " ('platform', 73),\n",
       " ('ye', 73),\n",
       " ('detail', 73),\n",
       " ('fb', 73),\n",
       " ('sxsw\\x89û\\x9d', 72),\n",
       " ('alreadi', 72),\n",
       " ('follow', 72),\n",
       " ('tomorrow', 72),\n",
       " ('place', 72),\n",
       " ('help', 71),\n",
       " ('block', 71),\n",
       " ('avail', 70)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we stem and remove stopwords from our data\n",
    "\n",
    "clean_tokens = stemmer(raw_tokens)\n",
    "clean_tokens = rem_stops(clean_tokens)\n",
    "\n",
    "frequency = FreqDist(clean_tokens)\n",
    "frequency.most_common(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we manually address some tokens in our bag, such as contractions and random noise.\n",
    "\n",
    "stops += ([\"rt\", \"'s\", \"n't\", \"'re\", \"'m\", \"'ll\", \"--\", \"..\",'link', 'mention', 'austin'] +\n",
    "         [str(i) for i in range(0, 10)])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('sxsw', 9402),\n",
       " ('googl', 2597),\n",
       " ('ipad', 2520),\n",
       " ('appl', 2303),\n",
       " ('quot', 1702),\n",
       " ('iphon', 1522),\n",
       " ('store', 1511),\n",
       " ('new', 1127),\n",
       " ('app', 1034),\n",
       " ('launch', 838),\n",
       " ('amp', 836),\n",
       " ('circl', 673),\n",
       " ('social', 650),\n",
       " ('thi', 613),\n",
       " ('today', 574),\n",
       " ('android', 572),\n",
       " ('open', 533),\n",
       " ('get', 529),\n",
       " ('network', 487),\n",
       " ('ipad2', 471),\n",
       " ('line', 453),\n",
       " ('go', 423),\n",
       " ('pop-up', 422),\n",
       " ('via', 413),\n",
       " ('call', 403),\n",
       " ('parti', 392),\n",
       " ('free', 387),\n",
       " ('mobil', 341),\n",
       " ('sxswi', 339),\n",
       " ('come', 331),\n",
       " ('like', 324),\n",
       " ('use', 319),\n",
       " ('ha', 313),\n",
       " ('time', 312),\n",
       " ('one', 312),\n",
       " ('major', 306),\n",
       " ('win', 299),\n",
       " ('map', 277),\n",
       " ('temporari', 264),\n",
       " ('day', 263),\n",
       " ('check', 259),\n",
       " ('possibl', 255),\n",
       " ('see', 254),\n",
       " ('need', 248),\n",
       " ('wa', 246),\n",
       " ('look', 234),\n",
       " ('design', 233),\n",
       " ('make', 227),\n",
       " ('peopl', 226),\n",
       " ('downtown', 225),\n",
       " ('great', 222),\n",
       " ('mayer', 215),\n",
       " ('popup', 210),\n",
       " ('know', 195),\n",
       " ('set', 194),\n",
       " ('talk', 192),\n",
       " ('marissa', 189),\n",
       " ('think', 189),\n",
       " ('want', 187),\n",
       " ('got', 185),\n",
       " ('show', 184),\n",
       " ('say', 182),\n",
       " ('w/', 182),\n",
       " ('onli', 175),\n",
       " ('good', 175),\n",
       " ('love', 174),\n",
       " ('thank', 174),\n",
       " ('\\x89ûï', 169),\n",
       " ('pop', 169),\n",
       " ('product', 167),\n",
       " ('first', 166),\n",
       " ('us', 162),\n",
       " ('take', 159),\n",
       " ('search', 157),\n",
       " ('year', 156),\n",
       " ('game', 156),\n",
       " ('befor', 156),\n",
       " ('panel', 156),\n",
       " ('user', 155),\n",
       " ('guy', 152),\n",
       " ('rumor', 150),\n",
       " ('music', 148),\n",
       " ('next', 147),\n",
       " ('cool', 147),\n",
       " ('shop', 146),\n",
       " ('tweet', 144),\n",
       " ('awesom', 144),\n",
       " ('tech', 144),\n",
       " ('best', 144),\n",
       " ('download', 143),\n",
       " ('work', 140),\n",
       " ('ani', 139),\n",
       " ('give', 137),\n",
       " ('video', 135),\n",
       " ('wait', 134),\n",
       " ('would', 134),\n",
       " ('big', 132),\n",
       " ('buy', 128),\n",
       " ('around', 127),\n",
       " ('last', 125),\n",
       " ('doe', 125),\n",
       " ('dure', 124),\n",
       " ('thing', 124),\n",
       " ('updat', 123),\n",
       " ('session', 122),\n",
       " ('case', 121),\n",
       " ('right', 120),\n",
       " ('anyon', 119),\n",
       " ('hi', 119),\n",
       " ('phone', 119),\n",
       " ('gt', 117),\n",
       " ('head', 117),\n",
       " ('whi', 117),\n",
       " ('news', 116),\n",
       " (\"'ve\", 115),\n",
       " ('even', 114),\n",
       " ('locat', 113),\n",
       " ('japan', 113),\n",
       " ('includ', 112),\n",
       " ('realli', 110),\n",
       " ('still', 107),\n",
       " ('photo', 106),\n",
       " ('start', 103),\n",
       " ('find', 103),\n",
       " ('live', 103),\n",
       " ('booth', 103),\n",
       " ('futur', 102),\n",
       " ('market', 102),\n",
       " ('team', 101),\n",
       " ('twitter', 101),\n",
       " ('event', 100),\n",
       " ('hey', 100),\n",
       " ('u', 100),\n",
       " ('congress', 100),\n",
       " ('way', 100),\n",
       " ('sell', 99),\n",
       " ('develop', 99),\n",
       " ('week', 99),\n",
       " ('itun', 98),\n",
       " ('blackberri', 98),\n",
       " ('digit', 96),\n",
       " ('world', 95),\n",
       " ('heard', 95),\n",
       " ('present', 95),\n",
       " ('tri', 94),\n",
       " ('could', 94),\n",
       " ('6th', 94),\n",
       " ('bing', 94),\n",
       " ('veri', 92),\n",
       " ('save', 92),\n",
       " ('let', 91),\n",
       " ('featur', 90),\n",
       " ('technolog', 90),\n",
       " ('everyon', 89),\n",
       " ('tonight', 88),\n",
       " ('may', 87),\n",
       " ('code', 87),\n",
       " ('play', 85),\n",
       " ('interest', 85),\n",
       " ('tv', 85),\n",
       " ('nice', 84),\n",
       " ('friend', 84),\n",
       " ('back', 84),\n",
       " ('meet', 83),\n",
       " ('fun', 83),\n",
       " ('interact', 82),\n",
       " ('facebook', 82),\n",
       " ('geek', 82),\n",
       " ('connect', 81),\n",
       " ('becaus', 81),\n",
       " ('ca', 80),\n",
       " ('lot', 80),\n",
       " ('also', 79),\n",
       " ('mani', 79),\n",
       " ('hour', 78),\n",
       " ('ever', 78),\n",
       " ('web', 78),\n",
       " ('releas', 78),\n",
       " ('away', 77),\n",
       " ('temp', 76),\n",
       " ('keep', 76),\n",
       " ('hope', 75),\n",
       " ('person', 75),\n",
       " ('pleas', 75),\n",
       " ('pic', 74),\n",
       " ('share', 74),\n",
       " ('sxsw.', 73),\n",
       " ('long', 73),\n",
       " ('platform', 73),\n",
       " ('ye', 73),\n",
       " ('detail', 73),\n",
       " ('fb', 73),\n",
       " ('sxsw\\x89û\\x9d', 72),\n",
       " ('alreadi', 72),\n",
       " ('follow', 72),\n",
       " ('tomorrow', 72),\n",
       " ('place', 72),\n",
       " ('help', 71),\n",
       " ('block', 71),\n",
       " ('avail', 70)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we inspect our final cleaned bag of words\n",
    "\n",
    "clean_tokens = stemmer(raw_tokens)\n",
    "clean_tokens = rem_stops(clean_tokens)\n",
    "\n",
    "frequency = FreqDist(clean_tokens)\n",
    "frequency.most_common(200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following Notebook, titled 'EDA', we will use this cleaned bag of words, as well as our cleaned dataframe to investigate data relationships. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we save our cleaned data to be used elsewhere. \n",
    "\n",
    "df.to_csv('data/intro_df.csv', index_label = 'id')\n",
    "\n",
    "tokens_df = pd.DataFrame(clean_tokens)\n",
    "tokens_df.to_csv('data/intro_tokens.csv', index_label = 'id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>product</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>.@wesley83 I have a 3G iPhone. After 3 hrs twe...</td>\n",
       "      <td>iPhone</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@jessedee Know about @fludapp ? Awesome iPad/i...</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@swonderlin Can not wait for iPad 2 also. They...</td>\n",
       "      <td>iPad</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@sxsw I hope this year's festival isn't as cra...</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@sxtxstate great stuff on Fri SXSW: Marissa Ma...</td>\n",
       "      <td>Google</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9088</th>\n",
       "      <td>Ipad everywhere. SXSW {link}</td>\n",
       "      <td>iPad</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9089</th>\n",
       "      <td>Wave, buzz... RT @mention We interrupt your re...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9090</th>\n",
       "      <td>Google's Zeiger, a physician never reported po...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9091</th>\n",
       "      <td>Some Verizon iPhone customers complained their...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9092</th>\n",
       "      <td>Ï¡Ïàü_ÊÎÒ£Áââ_£â_ÛâRT @...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9093 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text             product  \\\n",
       "0     .@wesley83 I have a 3G iPhone. After 3 hrs twe...              iPhone   \n",
       "1     @jessedee Know about @fludapp ? Awesome iPad/i...  iPad or iPhone App   \n",
       "2     @swonderlin Can not wait for iPad 2 also. They...                iPad   \n",
       "3     @sxsw I hope this year's festival isn't as cra...  iPad or iPhone App   \n",
       "4     @sxtxstate great stuff on Fri SXSW: Marissa Ma...              Google   \n",
       "...                                                 ...                 ...   \n",
       "9088                       Ipad everywhere. SXSW {link}                iPad   \n",
       "9089  Wave, buzz... RT @mention We interrupt your re...                 NaN   \n",
       "9090  Google's Zeiger, a physician never reported po...                 NaN   \n",
       "9091  Some Verizon iPhone customers complained their...                 NaN   \n",
       "9092  Ï¡Ïàü_ÊÎÒ£Áââ_£â_ÛâRT @...                 NaN   \n",
       "\n",
       "     sentiment  \n",
       "0     negative  \n",
       "1     positive  \n",
       "2     positive  \n",
       "3     negative  \n",
       "4     positive  \n",
       "...        ...  \n",
       "9088  positive  \n",
       "9089   neutral  \n",
       "9090   neutral  \n",
       "9091   neutral  \n",
       "9092   neutral  \n",
       "\n",
       "[9093 rows x 3 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>wesley83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3g</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>iphon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>tweet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98807</th>\n",
       "      <td>googl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98808</th>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98809</th>\n",
       "      <td>ûïcheck-in</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98810</th>\n",
       "      <td>offersû</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98811</th>\n",
       "      <td>sxsw</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>98812 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 0\n",
       "0         wesley83\n",
       "1               3g\n",
       "2            iphon\n",
       "3               hr\n",
       "4            tweet\n",
       "...            ...\n",
       "98807        googl\n",
       "98808         test\n",
       "98809  ûïcheck-in\n",
       "98810    offersû\n",
       "98811         sxsw\n",
       "\n",
       "[98812 rows x 1 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
